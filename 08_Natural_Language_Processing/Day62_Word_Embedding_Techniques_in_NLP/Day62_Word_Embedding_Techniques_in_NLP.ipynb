{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a520ba5-0206-4e89-b51c-c220201e71c9",
   "metadata": {},
   "source": [
    "# Day 62 – Word Embedding Algorithms: BoW, TF-IDF, and Word2Vec\n",
    "\n",
    "Today, I'm exploring **Word Embedding** algorithms. These techniques are essential because machine learning models only understand numbers, so we need effective ways to convert human language (text) into numerical vectors while preserving the meaning and context.\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "In Natural Language Processing (NLP), machines need a way to understand and process **text data** — but computers can’t read text directly.\n",
    "To make text understandable for algorithms, we must **convert it into numerical form**.\n",
    "\n",
    "This process is known as **Word Embedding** or **Text Vectorization**.\n",
    "Word embedding algorithms represent words as **vectors (numerical arrays)** that capture their meaning, frequency, and context.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Word Embeddings?**\n",
    "\n",
    "* Machine learning models work on **numerical input**, not raw text.\n",
    "* Word embeddings help convert **text → numbers** while keeping **semantic meaning** (relationships between words).\n",
    "* They help models understand:\n",
    "\n",
    "  * How often words appear\n",
    "  * How important they are in a document\n",
    "  * How words relate to each other\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Bag of Words (BoW)\n",
    "\n",
    "###  **Concept**\n",
    "\n",
    "The **Bag of Words (BoW)** model is the simplest way to convert text into numbers.\n",
    "It creates a **vocabulary** of all unique words in the dataset and represents each sentence as a **vector of word counts**.\n",
    "\n",
    "* Each column represents a word.\n",
    "* Each row represents a sentence or document.\n",
    "* The value shows how many times a word appeared in that sentence.\n",
    "\n",
    "> It ignores grammar, word order, and context — it only focuses on frequency.\n",
    "\n",
    "### **How it Works**\n",
    "\n",
    "1. Collect all unique words → create vocabulary\n",
    "2. For each sentence, count how many times each word appears\n",
    "3. Represent the counts in a table or matrix\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "Sentence 1: NLP makes machines understand text  \n",
    "Sentence 2: Machines learn from text data  \n",
    "```\n",
    "\n",
    "| Word       | Sentence 1 | Sentence 2 |\n",
    "| ---------- | ---------- | ---------- |\n",
    "| NLP        | 1          | 0          |\n",
    "| makes      | 1          | 0          |\n",
    "| machines   | 1          | 1          |\n",
    "| understand | 1          | 0          |\n",
    "| text       | 1          | 1          |\n",
    "| data       | 0          | 1          |\n",
    "| learn      | 0          | 1          |\n",
    "\n",
    "### Limitations:\n",
    "* **Sparsity**: If the vocabulary is large, most entries in the vector will be zero, leading to very large, inefficient \"sparse\" vectors.\n",
    "* **No Context/Meaning**: BoW completely ignores the **order** of words (the grammar) and treats all words equally. It doesn't capture semantic similarity (e.g., it treats \"king\" and \"apple\" as equally different).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa7dee94-de62-4edb-b60f-784be1bf13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b93c0f7b-440e-42ee-ac25-479973a1c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"Natural Language Processing makes machines understand text.\",\n",
    "    \"Machines can learn from text data using NLP techniques.\",\n",
    "    \"Text preprocessing is an important step in NLP.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c4afcf-95aa-43b8-a8df-411213d9be82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   an  can  data  from  important  in  is  language  learn  machines  makes  \\\n",
      "0   0    0     0     0          0   0   0         1      0         1      1   \n",
      "1   0    1     1     1          0   0   0         0      1         1      0   \n",
      "2   1    0     0     0          1   1   1         0      0         0      0   \n",
      "\n",
      "   natural  nlp  preprocessing  processing  step  techniques  text  \\\n",
      "0        1    0              0           1     0           0     1   \n",
      "1        0    1              0           0     0           1     1   \n",
      "2        0    1              1           0     1           0     1   \n",
      "\n",
      "   understand  using  \n",
      "0           1      0  \n",
      "1           0      1  \n",
      "2           0      0  \n"
     ]
    }
   ],
   "source": [
    "# Initialize CountVectorizer\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# Fit and transform sentences\n",
    "bow_matrix = cv.fit_transform(sentences)\n",
    "\n",
    "# Create a DataFrame to view results\n",
    "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=cv.get_feature_names_out())\n",
    "\n",
    "print(bow_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22a469d-6bf1-487c-bc40-a3c6288e4d36",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. TF-IDF (Term Frequency – Inverse Document Frequency)\n",
    "\n",
    "### **Concept**\n",
    "\n",
    "TF-IDF improves on BoW by not only counting words but also **weighing them** based on importance.\n",
    "It gives higher weight to **unique words** and lower weight to **common words** like “the”, “is”, etc.\n",
    "\n",
    "### **How it Works**\n",
    "\n",
    "1.  **TF (Term Frequency)** – measures how often a word appears in a document.\n",
    "\n",
    "    $$\n",
    "    \\text{TF} = \\frac{\\text{Number of times word appears}}{\\text{Total words in document}}\n",
    "    $$\n",
    "\n",
    "2.  **IDF (Inverse Document Frequency)** – measures how unique a word is across all documents.\n",
    "\n",
    "    $$\n",
    "    \\text{IDF} = \\log\\left(\\frac{\\text{Total documents}}{\\text{Documents containing the word}}\\right)\n",
    "    $$\n",
    "\n",
    "3.  **TF-IDF** is the final weight assigned to a word, calculated as the product of Term Frequency and Inverse Document Frequency:\n",
    "\n",
    "    $$\n",
    "    \\text{TF-IDF} = \\text{TF} \\times \\text{IDF}\n",
    "    $$\n",
    "\n",
    "This means:\n",
    "\n",
    "* Words appearing often in one document but rarely in others get **high scores**.\n",
    "* Common words appearing in all documents get **low scores**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10acaeda-4ddc-410a-a795-e86fa624c439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     an    can   data   from  important    in    is  language  learn  \\\n",
      "0  0.00  0.000  0.000  0.000       0.00  0.00  0.00     0.411  0.000   \n",
      "1  0.00  0.365  0.365  0.365       0.00  0.00  0.00     0.000  0.365   \n",
      "2  0.38  0.000  0.000  0.000       0.38  0.38  0.38     0.000  0.000   \n",
      "\n",
      "   machines  makes  natural    nlp  preprocessing  processing  step  \\\n",
      "0     0.312  0.411    0.411  0.000           0.00       0.411  0.00   \n",
      "1     0.278  0.000    0.000  0.278           0.00       0.000  0.00   \n",
      "2     0.000  0.000    0.000  0.289           0.38       0.000  0.38   \n",
      "\n",
      "   techniques   text  understand  using  \n",
      "0       0.000  0.243       0.411  0.000  \n",
      "1       0.365  0.216       0.000  0.365  \n",
      "2       0.000  0.224       0.000  0.000  \n"
     ]
    }
   ],
   "source": [
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform sentences\n",
    "tfidf_matrix = tfidf.fit_transform(sentences)\n",
    "\n",
    "# Create DataFrame to view results\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out())\n",
    "\n",
    "print(tfidf_df.round(3))  # round for better readability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d507c45e-dc37-46d2-a3cc-a7649b810d0b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Word2Vec\n",
    "\n",
    "### **Concept**\n",
    "\n",
    "Unlike BoW and TF-IDF, which treat words as isolated tokens, **Word2Vec** captures the **semantic relationships** between words.\n",
    "It converts each word into a **dense vector** (a list of numbers) that represents its **meaning and context**.\n",
    "\n",
    "This model is based on **neural networks** and learns relationships like:\n",
    "\n",
    "* “king” - “man” + “woman” ≈ “queen”\n",
    "* “Paris” - “France” + “Italy” ≈ “Rome”\n",
    "\n",
    "### **How it Works**\n",
    "\n",
    "Word2Vec uses one of two main approaches:\n",
    "\n",
    "1. **CBOW (Continuous Bag of Words)** – predicts a word from its surrounding context.\n",
    "2. **Skip-gram** – predicts surrounding words given a single target word.\n",
    "\n",
    "### **Key Features**\n",
    "\n",
    "* Learns **contextual meaning** of words\n",
    "* Produces **dense embeddings** (compact numeric representation)\n",
    "* Words with similar meanings have **similar vectors**\n",
    "\n",
    "### **Example with Gensim**\n",
    "\n",
    "We train Word2Vec on a small custom corpus.\n",
    "After training:\n",
    "\n",
    "* We can view the **vocabulary** (all words it learned)\n",
    "* We can find **similar words** (based on cosine similarity)\n",
    "* We can check the **vector representation** for any word\n",
    "\n",
    "Example result:\n",
    "\n",
    "```\n",
    "Words similar to 'learning':\n",
    "[('machine', 0.89), ('deep', 0.83), ('nlp', 0.78)]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94144ed2-dd02-4437-ad52-c2ea11de40cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Sample corpus (list of tokenized sentences)\n",
    "corpus = [\n",
    "    ['machine', 'learning', 'is', 'fun'],\n",
    "    ['deep', 'learning', 'is', 'a', 'part', 'of', 'machine', 'learning'],\n",
    "    ['nlp', 'stands', 'for', 'natural', 'language', 'processing'],\n",
    "    ['text', 'data', 'is', 'used', 'in', 'nlp'],\n",
    "    ['word2vec', 'creates', 'word', 'embeddings', 'for', 'text', 'data']\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = Word2Vec(sentences=corpus, vector_size=50, window=3, min_count=1, sg=0)\n",
    "\n",
    "# Get vocabulary\n",
    "print(\"Vocabulary:\", list(model.wv.index_to_key), \"\\n\")\n",
    "\n",
    "# Find similar words\n",
    "print(\"Words similar to 'learning':\")\n",
    "print(model.wv.most_similar('learning'))\n",
    "\n",
    "# Display word vector for one word\n",
    "print(\"\\nWord vector for 'machine':\")\n",
    "print(model.wv['machine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4056611-552d-4ed8-9764-0dfae39449ab",
   "metadata": {},
   "source": [
    "```\n",
    "Vocabulary: ['learning', 'is', 'for', 'data', 'nlp', 'text', 'machine', 'used', 'processing', 'language', 'natural', 'word', 'stands', 'in', 'of', 'part', 'a', 'deep', 'fun', 'word2vec', 'creates', 'embeddings'] \n",
    "\n",
    "Words similar to 'learning':\n",
    "[('fun', 0.27054551243782043), ('of', 0.2105627804994583), ('word', 0.16703936457633972), ('in', 0.15023992955684662), ('used', 0.13204482197761536), ('for', 0.12669815123081207), ('language', 0.09986130893230438), ('creates', 0.07065954059362411), ('a', 0.059388983994722366), ('deep', 0.04984808713197708)]\n",
    "\n",
    "Word vector for 'machine':\n",
    "[-0.01648536  0.01859871 -0.00039532 -0.00393455  0.00920726 -0.00819063\n",
    "  0.00548623  0.01387993  0.01213085 -0.01502159  0.0187647   0.00934362\n",
    "  0.00793224 -0.01248701  0.01691996 -0.00430033  0.01765038 -0.01072401\n",
    " -0.01625884  0.01364912  0.00334239 -0.00439702  0.0190272   0.01898771\n",
    " -0.01954809  0.00501046  0.01231338  0.00774491  0.00404557  0.000861\n",
    "  0.00134726 -0.00764127 -0.0142805  -0.00417774  0.0078478   0.01763737\n",
    "  0.0185183  -0.01195187 -0.01880534  0.01952875  0.00685957  0.01033223\n",
    "  0.01256469 -0.00560853  0.01464541  0.00566054  0.00574201 -0.00476074\n",
    " -0.0062565  -0.00474028]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24463873-44d4-4786-86b0-5f125016e9d3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Comparison Summary**\n",
    "\n",
    "| Feature           | BoW           | TF-IDF                 | Word2Vec        |\n",
    "| ----------------- | ------------- | ---------------------- | --------------- |\n",
    "| Captures meaning  | No            | Partially              | Yes             |\n",
    "| Context awareness | No            | No                     | Yes             |\n",
    "| Output type       | Sparse matrix | Sparse matrix          | Dense vectors   |\n",
    "| Based on          | Frequency     | Frequency + Importance | Neural networks |\n",
    "| Common use        | Simple models | Text classification    | Deep NLP models |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa7791-826d-41f2-93e6-71f9051b71a5",
   "metadata": {},
   "source": [
    "## **Conclusion**\n",
    "\n",
    "In this session, I explored one of the most important concepts in Natural Language Processing — **Word Embeddings**, which help convert text data into numerical form that machines can understand.\n",
    "\n",
    "I started by revisiting simple methods like **Bag of Words (BoW)** and **TF-IDF**, which focus on word frequency and importance, and then moved towards **Word2Vec**, which captures the *semantic meaning* and *contextual relationship* between words.\n",
    "\n",
    "Each algorithm progressively improves the way textual information is represented:\n",
    "\n",
    "* BoW focuses only on **counting words**.\n",
    "* TF-IDF adds a measure of **importance** to each word.\n",
    "* Word2Vec introduces **context awareness**, helping machines understand word relationships and meanings.\n",
    "\n",
    "These techniques form the foundation of modern NLP systems. Advanced language models like **BERT**, **GPT**, and **T5** are built upon these concepts — they use contextual embeddings to understand language at a much deeper level.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Takeaways**\n",
    "\n",
    "* **Word embeddings** are numerical representations of words that help machines understand text.\n",
    "\n",
    "* **Bag of Words (BoW):**\n",
    "\n",
    "  * Converts text into word count vectors.\n",
    "  * Ignores grammar and word order.\n",
    "  * Simple but effective for basic models.\n",
    "\n",
    "* **TF-IDF (Term Frequency – Inverse Document Frequency):**\n",
    "\n",
    "  * Weighs words based on their frequency and uniqueness.\n",
    "  * Common words get lower weights, rare words get higher weights.\n",
    "  * Useful for text classification and document similarity tasks.\n",
    "\n",
    "* **Word2Vec:**\n",
    "\n",
    "  * Neural network-based algorithm that captures semantic meaning.\n",
    "  * Words appearing in similar contexts have similar vector representations.\n",
    "  * Forms the basis for modern embedding-based NLP models.\n",
    "\n",
    "* **Progression Summary:**\n",
    "  BoW ➜ TF-IDF ➜ Word2Vec represents the evolution from **frequency-based** to **context-based** understanding of text.\n",
    "\n",
    "* These embedding techniques are crucial before feeding text data into **Machine Learning** or **Deep Learning** models for NLP tasks like sentiment analysis, chatbots, and text summarization.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
