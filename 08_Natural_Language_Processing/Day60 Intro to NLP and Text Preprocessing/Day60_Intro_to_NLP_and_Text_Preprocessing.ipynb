{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6b0c784-a2d0-4940-bc5a-69cd65e587fc",
   "metadata": {},
   "source": [
    "# Day 60 – Introduction to Natural Language Processing (NLP) \n",
    "\n",
    "Today, I am starting the journey into **Artificial Intelligence (AI)**. I've successfully completed the core **Machine Learning (ML)** part, which focused heavily on structured data. Now, I will shift my focus to **unstructured data** and the technologies that allow machines to understand it.\n",
    "\n",
    "AI is a broader concept that focuses on building systems capable of performing tasks that typically require **human intelligence** — such as understanding language, recognizing images, making decisions, or learning from experience.\n",
    "\n",
    "---\n",
    "\n",
    "## 1. The World of Artificial Intelligence (AI)\n",
    "\n",
    "> **Artificial Intelligence (AI)** is the concept of creating machines and computer systems that can mimic or simulate human intelligence.\n",
    "\n",
    "While Machine Learning focuses on learning patterns from data, AI encompasses a broader range of tasks, including:\n",
    "\n",
    "  * Reasoning and problem-solving.\n",
    "  * Perception (via image and audio data).\n",
    "  * Language understanding and generation.\n",
    "\n",
    "AI systems often work with unstructured data, which includes:\n",
    "\n",
    "  * **Text Data** (the focus of NLP)\n",
    "  * **Image, Audio, and Video**\n",
    "  * **PDFs, XML/HTML documents**\n",
    "  * Sensor data and drone data\n",
    "\n",
    "\n",
    "### Types of AI Data and Focus Areas\n",
    "\n",
    "When dealing with **unstructured data**, AI uses different specialized subfields:\n",
    "\n",
    "| Type of Data         | AI Domain / Technique                                                                          |\n",
    "| -------------------- | ---------------------------------------------------------------------------------------------- |\n",
    "| **Text Data**        | NLP (Natural Language Processing), RNN (Recurrent Neural Network), LLM (Large Language Models) |\n",
    "| **Image Data**       | Computer Vision, CNNs                                                                          |\n",
    "| **Audio/Video Data** | Speech Recognition, Audio Processing, Deep Learning                                            |\n",
    "\n",
    "In this notebook, I begin with the **Text Data** part — that is, **Natural Language Processing (NLP)**.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Natural Language Processing (NLP)\n",
    "\n",
    "> **Natural Language Processing (NLP)** is a branch of Artificial Intelligence that helps computers understand, interpret, and generate human language.\n",
    "\n",
    "It bridges the gap between **human communication** and **machine understanding** — enabling machines to process text and speech in the way humans do.\n",
    "\n",
    "### How NLP Works\n",
    "\n",
    "NLP works by breaking down text into smaller, meaningful pieces and applying linguistic and statistical techniques to extract meaning.\n",
    "\n",
    "A general NLP workflow includes:\n",
    "\n",
    "1. **Text Preprocessing** – Cleaning, tokenizing, stemming, lemmatizing text.\n",
    "2. **Feature Extraction** – Converting text into numerical features (Bag of Words, TF-IDF, Word2Vec, etc.).\n",
    "3. **Model Building** – Applying ML or DL models for tasks like classification or translation.\n",
    "4. **Evaluation** – Measuring performance using metrics such as accuracy, precision, recall, etc.\n",
    "5. **Deployment** – Using trained NLP models in applications like chatbots or recommendation systems.\n",
    "\n",
    "### Applications of NLP\n",
    "\n",
    "NLP is widely used in various AI applications, including:\n",
    "\n",
    "*  **Text Classification** – Spam filtering, sentiment analysis, topic detection\n",
    "*  **Chatbots & Virtual Assistants** – Siri, Alexa, ChatGPT\n",
    "*  **Information Retrieval** – Search engines like Google\n",
    "*  **Text Summarization** – Automatic summary generation\n",
    "*  **Machine Translation** – Google Translate\n",
    "*  **Named Entity Recognition (NER)** – Identifying names, locations, organizations, etc.\n",
    "\n",
    "### Components of NLP\n",
    "\n",
    "NLP consists of two main components:\n",
    "\n",
    "### 1. **NLU (Natural Language Understanding)**\n",
    "\n",
    "Focuses on understanding human language and extracting meaning.\n",
    "Tasks include:\n",
    "\n",
    "* Intent Detection\n",
    "* Entity Recognition\n",
    "* Sentiment Analysis\n",
    "* Semantic Interpretation\n",
    "\n",
    "### 2. **NLG (Natural Language Generation)**\n",
    "\n",
    "Focuses on generating text that sounds natural to humans.\n",
    "Tasks include:\n",
    "\n",
    "* Chatbot response generation\n",
    "* Report writing\n",
    "* Text summarization\n",
    "* Story or content creation\n",
    "\n",
    "\n",
    "### Key NLP Libraries\n",
    "\n",
    "While libraries like `scikit-learn` and `XGBoost` are used for the ML framework, NLP has its own set of specialized libraries for handling text data:\n",
    "\n",
    "  * **NLTK (Natural Language Toolkit)**: A widely used, older, foundational library providing access to many corpora and lexical resources. It's often used for academic and research purposes.\n",
    "  * **spaCy**: A newer, more production-ready library known for its speed and efficiency, often used for industrial-scale NLP applications.\n",
    "  * **Gensim**: A robust library primarily focused on topic modeling and document similarity (e.g., Word2Vec).\n",
    "  * **Stanford NLP** (e.g., CoreNLP): A suite of high-quality tools for advanced language analysis (parsing, named entity recognition).\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Hierarchy of Text in NLP\n",
    "\n",
    "Text data follows a specific hierarchy, from the smallest unit to the largest:\n",
    "\n",
    "  * **Word**: The smallest meaningful unit of text.\n",
    "  * **Collection of Words** = **Sentence**: A grammatically complete thought.\n",
    "  * **Collection of Sentences** = **Paragraph**: A group of sentences forming an idea  \n",
    "  * **Collection of Paragraphs** = **Document**: A complete article, email, or report.\n",
    "  * **Collection of Documents** = **Articles/Corpus**: A large body of text data used for training.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Text Preprocessing: Tokenization, Stemming and Lemmatization\n",
    "\n",
    "Before any NLP model can be trained, raw text must be cleaned and converted into a usable format through preprocessing steps like tokenization and stemming.\n",
    "\n",
    "\n",
    "## 4.1 Tokenization\n",
    "\n",
    "#### What is Tokenization?\n",
    "> Tokenization is the process of breaking text into smaller pieces called **tokens** (words, sentences, or paragraphs).\n",
    "\n",
    "It is the **first and most important step** in any NLP pipeline, helping computers understand the structure of text data.\n",
    "\n",
    "\n",
    "#### Types of Tokenization (using NLTK)\n",
    "\n",
    "| Tokenization Type | Description | NLTK Function |\n",
    "| :--- | :--- | :--- |\n",
    "| **Word Tokenize** | Splits text into individual words and punctuation. | `word_tokenize` |\n",
    "| **Sentence Tokenize** | Splits text into individual sentences. | `sent_tokenize` |\n",
    "| **Blankline Tokenize** | Splits a document into paragraphs (tokens separated by blank lines). | `blankline_tokenize` |\n",
    "| **Whitespace Tokenizer** | Splits text only by white space, retaining punctuation attached to words. | `WhitespaceTokenizer` |\n",
    "| **WordPunct Tokenizer** | Splits on white space and separates all punctuation from words. | `WordPunctTokenizer` |\n",
    "\n",
    "You can explore the official NLTK documentation here:\n",
    "🔗 [https://www.nltk.org/api/nltk.tokenize.html](https://www.nltk.org/api/nltk.tokenize.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c304da83-5256-4449-9ba1-d1b36646c435",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7dfe4c-424f-454d-ad38-ad58bc0e6dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "AI = '''Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of \n",
    "humans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and \n",
    "problem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines. \n",
    "It is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\n",
    "AI could solve major challenges and crisis situations.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154d7bd1-5f95-49e7-b57d-c353611d34a8",
   "metadata": {},
   "source": [
    "### Word Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7304d1a-ebdd-4af4-b69b-b9454782aba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac73deec-fb13-488e-9488-b562329c6464",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', 'refers', 'to', 'the', 'intelligence', 'of', 'machines', '.', 'This', 'is', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'of', 'humans', 'and', 'animals', '.', 'With', 'Artificial', 'Intelligence', ',', 'machines', 'perform', 'functions', 'such', 'as', 'learning', ',', 'planning', ',', 'reasoning', 'and', 'problem-solving', '.', 'Most', 'noteworthy', ',', 'Artificial', 'Intelligence', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines', '.', 'It', 'is', 'probably', 'the', 'fastest-growing', 'development', 'in', 'the', 'World', 'of', 'technology', 'and', 'innovation', '.', 'Furthermore', ',', 'many', 'experts', 'believe', 'AI', 'could', 'solve', 'major', 'challenges', 'and', 'crisis', 'situations', '.']\n"
     ]
    }
   ],
   "source": [
    "AI_tokens = word_tokenize(AI)\n",
    "print(AI_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "685fb228-4ce9-402d-888b-26cdb5ba928b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57a611d-94d6-4535-b0f5-a60c3e33c693",
   "metadata": {},
   "source": [
    "### Sentence Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e26b99b-e804-46f4-9a34-14c949b56514",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06374fd7-972a-4ac4-b64c-844baa52017f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence refers to the intelligence of machines.',\n",
       " 'This is in contrast to the natural intelligence of \\nhumans and animals.',\n",
       " 'With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and \\nproblem-solving.',\n",
       " 'Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines.',\n",
       " 'It is probably the fastest-growing development in the World of technology and innovation.',\n",
       " 'Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AI_sent = sent_tokenize(AI)\n",
    "AI_sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83827e40-9880-4b76-a12a-9bdcd54c0f78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d2acd2-dceb-41e6-b6f3-f914939b2f9f",
   "metadata": {},
   "source": [
    "### Blankline Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1cb816f-a247-4f4e-815c-694de94a1476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Artificial Intelligence refers to the intelligence of machines. This is in contrast to the natural intelligence of \\nhumans and animals. With Artificial Intelligence, machines perform functions such as learning, planning, reasoning and \\nproblem-solving. Most noteworthy, Artificial Intelligence is the simulation of human intelligence by machines. \\nIt is probably the fastest-growing development in the World of technology and innovation. Furthermore, many experts believe\\nAI could solve major challenges and crisis situations.']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import blankline_tokenize \n",
    "AI_blank = blankline_tokenize(AI) \n",
    "AI_blank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "19d78c7e-8018-4cc2-bfa4-c5c4e2b6c97c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(AI_blank) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc917525-3b57-4cde-8473-ca6e275ef539",
   "metadata": {},
   "source": [
    "### Whitespace Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7ba0f3d8-7bd3-4a14-a87f-4fe26e40b662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', 'refers', 'to', 'the', 'intelligence', 'of', 'machines.', 'This', 'is', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'of', 'humans', 'and', 'animals.', 'With', 'Artificial', 'Intelligence,', 'machines', 'perform', 'functions', 'such', 'as', 'learning,', 'planning,', 'reasoning', 'and', 'problem-solving.', 'Most', 'noteworthy,', 'Artificial', 'Intelligence', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines.', 'It', 'is', 'probably', 'the', 'fastest-growing', 'development', 'in', 'the', 'World', 'of', 'technology', 'and', 'innovation.', 'Furthermore,', 'many', 'experts', 'believe', 'AI', 'could', 'solve', 'major', 'challenges', 'and', 'crisis', 'situations.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "wt = WhitespaceTokenizer().tokenize(AI)\n",
    "print(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b931e141-32d6-4ee8-8485-e82d6c96cb0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "70"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d75ba60-258d-414a-8097-0f4ca57e8705",
   "metadata": {},
   "source": [
    "### WordPunct Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e9641d1-a7e3-4046-9344-20540ef71163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Good apple cost $3.88 in Hyderabad. Please buy two of them. Thanks.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "s = 'Good apple cost $3.88 in Hyderabad. Please buy two of them. Thanks.'\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bfcb049b-263f-4f3f-92d2-233c574b35fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'apple', 'cost', '$', '3', '.', '88', 'in', 'Hyderabad', '.', 'Please', 'buy', 'two', 'of', 'them', '.', 'Thanks', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wordpunct_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b7b8c7e-4f2a-45bb-8e51-d56df31c692f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordpunct_tokenize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "649e5e91-d92e-4fbd-a4d8-490e23a175de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Artificial', 'Intelligence', 'refers', 'to', 'the', 'intelligence', 'of', 'machines', '.', 'This', 'is', 'in', 'contrast', 'to', 'the', 'natural', 'intelligence', 'of', 'humans', 'and', 'animals', '.', 'With', 'Artificial', 'Intelligence', ',', 'machines', 'perform', 'functions', 'such', 'as', 'learning', ',', 'planning', ',', 'reasoning', 'and', 'problem', '-', 'solving', '.', 'Most', 'noteworthy', ',', 'Artificial', 'Intelligence', 'is', 'the', 'simulation', 'of', 'human', 'intelligence', 'by', 'machines', '.', 'It', 'is', 'probably', 'the', 'fastest', '-', 'growing', 'development', 'in', 'the', 'World', 'of', 'technology', 'and', 'innovation', '.', 'Furthermore', ',', 'many', 'experts', 'believe', 'AI', 'could', 'solve', 'major', 'challenges', 'and', 'crisis', 'situations', '.']\n"
     ]
    }
   ],
   "source": [
    "w_p = wordpunct_tokenize(AI)\n",
    "print(w_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aa78e2c4-925b-4985-9d08-3edcfa4fb585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(w_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045c2144-496f-41d9-84b5-0d986590f1f2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.2 Understanding N-Grams\n",
    "\n",
    "While tokenization breaks text into individual words or sentences, sometimes analyzing **combinations of consecutive words** provides better context.  \n",
    "This is where **N-Grams** come in.\n",
    "\n",
    "| Type | Description | Example (for sentence: “Natural Language Processing is fun”) |\n",
    "|------|--------------|---------------------------------------------------------------|\n",
    "| **Unigram** | Single word | `[\"Natural\", \"Language\", \"Processing\", \"is\", \"fun\"]` |\n",
    "| **Bigram** | Pair of two consecutive words | `[(\"Natural\", \"Language\"), (\"Language\", \"Processing\"), ...]` |\n",
    "| **Trigram** | Sequence of three consecutive words | `[(\"Natural\", \"Language\", \"Processing\"), (\"Language\", \"Processing\", \"is\"), ...]` |\n",
    "| **N-Gram** | Sequence of *n* words | Generalized form of above |\n",
    "\n",
    "These are helpful in identifying **common patterns and word relationships**, especially in text classification and language modeling tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4af9958-efba-46ab-a41b-23072ce6e13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import bigrams,trigrams,ngrams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bdb151ae-32da-4293-b6af-7d7eddea15ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the best and most beautifull thing in the world cannot be seen or even touched,they must be felt with heart'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = 'the best and most beautifull thing in the world cannot be seen or even touched,they must be felt with heart'\n",
    "string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "56def6b4-fea0-4dfd-afe5-ae6e8d0f9458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'best', 'and', 'most', 'beautifull', 'thing', 'in', 'the', 'world', 'can', 'not', 'be', 'seen', 'or', 'even', 'touched', ',', 'they', 'must', 'be', 'felt', 'with', 'heart']\n"
     ]
    }
   ],
   "source": [
    "quotes_tokens = nltk.word_tokenize(string)\n",
    "print(quotes_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5df1dd14-c43e-4057-bc9c-73310ca8227b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(quotes_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ef1a35-89f3-4d40-bab0-2c18ed5e81d0",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9449f13f-f180-45d5-bfb3-c426b2cd112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'best'), ('best', 'and'), ('and', 'most'), ('most', 'beautifull'), ('beautifull', 'thing'), ('thing', 'in'), ('in', 'the'), ('the', 'world'), ('world', 'can'), ('can', 'not'), ('not', 'be'), ('be', 'seen'), ('seen', 'or'), ('or', 'even'), ('even', 'touched'), ('touched', ','), (',', 'they'), ('they', 'must'), ('must', 'be'), ('be', 'felt'), ('felt', 'with'), ('with', 'heart')]\n"
     ]
    }
   ],
   "source": [
    "quotes_bigrams = list(nltk.bigrams(quotes_tokens))\n",
    "print(quotes_bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6520f46f-3d06-499b-9cc0-51a98157b9a6",
   "metadata": {},
   "source": [
    "### Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dafafe1b-8bf4-4186-8b35-a78c249aedbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'best', 'and'), ('best', 'and', 'most'), ('and', 'most', 'beautifull'), ('most', 'beautifull', 'thing'), ('beautifull', 'thing', 'in'), ('thing', 'in', 'the'), ('in', 'the', 'world'), ('the', 'world', 'can'), ('world', 'can', 'not'), ('can', 'not', 'be'), ('not', 'be', 'seen'), ('be', 'seen', 'or'), ('seen', 'or', 'even'), ('or', 'even', 'touched'), ('even', 'touched', ','), ('touched', ',', 'they'), (',', 'they', 'must'), ('they', 'must', 'be'), ('must', 'be', 'felt'), ('be', 'felt', 'with'), ('felt', 'with', 'heart')]\n"
     ]
    }
   ],
   "source": [
    "quotes_trigrams = list(nltk.trigrams(quotes_tokens))\n",
    "print(quotes_trigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b34ee166-bdc1-49f0-bb57-818eb6508fba",
   "metadata": {},
   "source": [
    "### n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "35319f1d-03e2-49e6-8f58-f0dd9f2716bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 'best', 'and', 'most'), ('best', 'and', 'most', 'beautifull'), ('and', 'most', 'beautifull', 'thing'), ('most', 'beautifull', 'thing', 'in'), ('beautifull', 'thing', 'in', 'the'), ('thing', 'in', 'the', 'world'), ('in', 'the', 'world', 'can'), ('the', 'world', 'can', 'not'), ('world', 'can', 'not', 'be'), ('can', 'not', 'be', 'seen'), ('not', 'be', 'seen', 'or'), ('be', 'seen', 'or', 'even'), ('seen', 'or', 'even', 'touched'), ('or', 'even', 'touched', ','), ('even', 'touched', ',', 'they'), ('touched', ',', 'they', 'must'), (',', 'they', 'must', 'be'), ('they', 'must', 'be', 'felt'), ('must', 'be', 'felt', 'with'), ('be', 'felt', 'with', 'heart')]\n"
     ]
    }
   ],
   "source": [
    "quotes_ngrams = list(nltk.ngrams(quotes_tokens, 4)) \n",
    "print(quotes_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbb422e-37a7-42b3-ab80-c63e1c4686ae",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4.3 Stemming\n",
    "\n",
    "> **Stemming** is the process of reducing a word to its **root or base form**.\n",
    "\n",
    "  * **Example**: The words \"running,\" \"runs,\" and \"ran\" are all reduced to the stem \"run.\"\n",
    "  * **Note**: The stem may not be a valid word itself.\n",
    "\n",
    "Stemming helps reduce variations of a word to a common root, simplifying analysis.\n",
    "\n",
    "###  Common Stemmer Algorithms in NLTK\n",
    "\n",
    "| Stemmer              | Description                                                                        |\n",
    "| -------------------- | ---------------------------------------------------------------------------------- |\n",
    "| **PorterStemmer**    | Most common stemmer; balances accuracy and simplicity.                             |\n",
    "| **LancasterStemmer** | More aggressive; may over-stem words.                                              |\n",
    "| **SnowballStemmer**  | An improved and versatile version of Porter stemmer supporting multiple languages. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386fa497-3379-4571-b5bd-29aac9379058",
   "metadata": {},
   "source": [
    "### Porter Stemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f30031dd-58e2-4395-9615-0f60b77ebf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "pst = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "edbceec4-967e-4f74-afc0-fa7c8677f3ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'give'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem('having')\n",
    "pst.stem('affection')\n",
    "pst.stem('playing')\n",
    "pst.stem('give') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "942f900c-6c78-410d-87ea-5453c107c1b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "given : given\n",
      "gave : gave\n",
      "thinking : think\n",
      "loving : love\n",
      "final : final\n",
      "finalized : final\n",
      "finally : final\n"
     ]
    }
   ],
   "source": [
    "words_to_stem=['give','giving','given','gave','thinking', 'loving', 'final', 'finalized', 'finally']\n",
    "for words in words_to_stem:\n",
    "    print(words+  ' : ' + pst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea0eb9-cfd4-4e77-82b0-e19bef54d389",
   "metadata": {},
   "source": [
    "### Lancaster Stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8456be8-2436-43c1-bfea-c21cfea395cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : giv\n",
      "giving : giv\n",
      "given : giv\n",
      "gave : gav\n",
      "thinking : think\n",
      "loving : lov\n",
      "final : fin\n",
      "finalized : fin\n",
      "finally : fin\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "lst = LancasterStemmer()\n",
    "for words in words_to_stem:\n",
    "    print(words + ' : ' + lst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533c7132-6f20-4cca-8d2d-f75aaa090b53",
   "metadata": {},
   "source": [
    "### Snowball Stemmer (English)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f7c57b4f-38c9-4c1d-99b4-a3b05bb35e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : give\n",
      "given : given\n",
      "gave : gave\n",
      "thinking : think\n",
      "loving : love\n",
      "final : final\n",
      "finalized : final\n",
      "finally : final\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "sbst = SnowballStemmer('english')\n",
    "\n",
    "for words in words_to_stem:\n",
    "    print(words+ ' : ' +sbst.stem(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b987451d-10dd-4193-80b0-5806b8689c1e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4.4 Lemmatization\n",
    "\n",
    "> **Lemmatization** is the process of reducing a word to its base or dictionary form, known as the **lemma**.\n",
    "\n",
    "Just like **stemming**, lemmatization is also a technique used to reduce words to their **base or root form**.  \n",
    "However, the key difference is that **lemmatization is more intelligent and linguistically accurate** — it uses a **dictionary** and **part-of-speech (POS)** understanding to derive the *correct* root form.\n",
    "For example:\n",
    "\n",
    "| Word | Lemmatized Form |\n",
    "|-------|------------------|\n",
    "| running | run |\n",
    "| studies | study |\n",
    "| better | good |\n",
    "\n",
    "Unlike stemming, which may simply cut off word endings, lemmatization considers **the context and grammar** to return a proper word.\n",
    "\n",
    "### Stemming vs. Lemmatization\n",
    "\n",
    "| Aspect | Stemming | Lemmatization |\n",
    "|---------|-----------|----------------|\n",
    "| Approach | Rule-based (cuts suffixes) | Dictionary-based (uses linguistic analysis) |\n",
    "| Output | May produce non-words (e.g., *“stud”* for *“studies”*) | Always produces valid words |\n",
    "| Speed | Faster | Slower |\n",
    "| Accuracy | Lower | Higher |\n",
    "\n",
    "### Why Use Lemmatization?\n",
    "\n",
    "- Ensures all variations of a word are treated as one (e.g., *run, running, ran → run*)  \n",
    "- Improves accuracy in NLP models  \n",
    "- Essential for tasks like **sentiment analysis, document classification, and information retrieval**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "02be294c-5bba-477e-a137-2a5f90f50cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "give : give\n",
      "giving : giving\n",
      "given : given\n",
      "gave : gave\n",
      "thinking : thinking\n",
      "loving : loving\n",
      "final : final\n",
      "finalized : finalized\n",
      "finally : finally\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "word_lem = WordNetLemmatizer()\n",
    "\n",
    "for word in words_to_stem:\n",
    "    print(word + ' : ' + word_lem.lemmatize(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db00f38-d425-4745-8f48-27906cf6e58f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, I officially began my journey into **Artificial Intelligence (AI)** by exploring one of its most impactful branches — **Natural Language Processing (NLP)**.  \n",
    "After completing the Machine Learning section, this marks a major step toward working with **unstructured data**, especially **text data**.\n",
    "\n",
    "I learned how computers process, understand, and generate human language using various techniques.  \n",
    "Starting from **tokenization** (splitting text into meaningful units) to **stemming** and **lemmatization** (reducing words to their root form), each concept plays a foundational role in preparing text for NLP models.\n",
    "\n",
    "These text preprocessing steps are essential before applying advanced NLP methods such as **TF-IDF, Word2Vec, Transformers, and LLMs**.\n",
    "\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **AI** is about mimicking human intelligence — it deals with unstructured data such as text, audio, and images.  \n",
    "- **NLP (Natural Language Processing)** helps machines understand and process human language.  \n",
    "- **Tokenization** breaks down text into smaller components (words, sentences, paragraphs) for analysis.  \n",
    "- **Stemming** simplifies words to their root form but may produce non-words (e.g., *stud*).  \n",
    "- **Lemmatization** provides dictionary-based, accurate base forms of words (e.g., *studies → study*).  \n",
    "- These preprocessing techniques are the **first step** toward building powerful NLP pipelines.  \n",
    "- Libraries such as **NLTK**, **spaCy**, **Gensim**, and **Stanford NLP** are essential tools in this domain.  \n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
