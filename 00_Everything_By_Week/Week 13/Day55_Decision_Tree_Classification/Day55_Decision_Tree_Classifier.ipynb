{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c69a3ae2-6e22-40de-bf43-351c630cfbc6",
   "metadata": {},
   "source": [
    "# Day 55 - Decision Tree Classifier\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, I focus on the **Decision Tree Classifier**, a supervised learning algorithm that splits data into branches based on feature values to make predictions.  \n",
    "\n",
    "I begin with a short theory on how decision trees work, covering key concepts such as **entropy, information gain, Gini impurity, and pruning**. \n",
    "\n",
    "Then I implement a Decision Tree classifier on a dataset, training and evaluating models:  \n",
    "- With and without scaling (to show that scaling is not necessary for decision trees)  \n",
    "- With different tree depths (`max_depth=3` and `max_depth=10`) to compare performance and generalization  \n",
    "\n",
    "By the end of this notebook, it becomes clear why decision trees are scale-invariant, how tree depth impacts overfitting vs generalization, and how pruning parameters help control model complexity.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48199ff7-a5e2-46bf-a2cf-3284d989ced0",
   "metadata": {},
   "source": [
    "## 1. Decision Tree Classifier \n",
    "\n",
    "A **Decision Tree** is a supervised machine learning algorithm that is both powerful and intuitive. It models decisions by splitting data into subsets based on the values of its features, creating a flowchart-like structure. The goal is to build a tree that can predict a target variable by asking a series of questions.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1 Entropy\n",
    "\n",
    "**Entropy** is a measure of the impurity or randomness of a dataset. In the context of a decision tree, a node has high entropy if its data points are a mix of different classes (e.g., half 'Up' and half 'Down'). A node has zero entropy if all its data points belong to the same class, meaning it is perfectly pure.\n",
    "\n",
    "The formula for Entropy is:\n",
    "\n",
    "$$E(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "  * $S$ is a set of training examples.\n",
    "  * $c$ is the number of classes.\n",
    "  * $p_i$ is the proportion of examples belonging to class $i$.\n",
    "\n",
    "### 1.2 Gini Index (Gini Impurity)\n",
    "\n",
    "The **Gini Index** is an alternative to Entropy for measuring impurity. A Gini Index of 0 means the node is perfectly pure (all data points are of the same class), while a Gini Index close to 1 means the node is highly impure. The algorithm seeks to minimize Gini Impurity at each split.\n",
    "\n",
    "The formula for the Gini Index is:\n",
    "\n",
    "$$Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
    "\n",
    "Both Entropy and Gini Index achieve the same goal, but the Gini Index is often preferred in practice as it is computationally faster.\n",
    "\n",
    "### 1.3 Information Gain (IG)\n",
    "\n",
    "**Information Gain (IG)** is the main criterion used by the decision tree algorithm to select the best feature for a split. It measures the reduction in entropy (or impurity) achieved by splitting the data on a particular feature. The algorithm always chooses the feature with the **highest Information Gain** to make the split, as this results in the purest possible child nodes.\n",
    "\n",
    "The formula for Information Gain is:\n",
    "\n",
    "$$IG(S, A) = E(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} E(S_v)$$\n",
    "\n",
    "Where:\n",
    "\n",
    "  * $S$ is the dataset before the split.\n",
    "  * $A$ is the feature being evaluated for the split.\n",
    "  * $Values(A)$ is the set of all unique values in feature $A$.\n",
    "  * $S_v$is the subset of $S$ where feature $A$ has the value $v$.\n",
    "  * $|S_v|$ and $|S|$ are the number of elements in the respective sets.\n",
    "\n",
    "### 1.4 Pruning\n",
    "\n",
    "**Pruning** is a technique used to combat **overfitting** in a decision tree. A tree that is too deep can become overly complex, memorizing the training data instead of learning general patterns. Pruning simplifies the tree by removing branches that are not essential.\n",
    "\n",
    "There are two main types of pruning:\n",
    "\n",
    "  * **Pre-pruning**: Stopping the tree from growing before it's fully developed by setting hyperparameters like `max_depth` or `min_samples_leaf`.\n",
    "  * **Post-pruning**: Growing a full tree and then cutting back branches that do not contribute significantly to the model's performance.\n",
    "\n",
    "**Common pruning parameters in scikit-learn:**\n",
    "- `max_depth`: maximum number of levels in the tree  \n",
    "- `min_samples_split`: minimum samples required to split a node  \n",
    "- `min_samples_leaf`: minimum samples required at a leaf node  \n",
    "- `max_leaf_nodes`: maximum number of leaves  \n",
    "\n",
    "These parameters control complexity and improve generalization.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Steps to Build a Decision Tree\n",
    "\n",
    "The process of building a decision tree from a dataset based on Information Gain follows these steps:\n",
    "\n",
    "1.  **Understand the Dataset**: Analyze the features (independent variables) and the target (dependent variable).\n",
    "2.  **Calculate Total Entropy**: Calculate the entropy of the target variable to understand the overall impurity of the dataset.\n",
    "3.  **Calculate Information Gain**: For each feature, calculate its Information Gain.\n",
    "4.  **Select the Root Node**: Choose the feature with the highest Information Gain to be the root node of the tree.\n",
    "5.  **Repeat**: Repeat the process for each branch until all leaf nodes are pure (entropy is 0) or a stopping criterion is met (e.g., `max_depth`).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577ef61-7f99-45ef-9ac3-29f3b1ed1715",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a485cafe-9803-4d91-a986-55efc50e0fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ee3c26-4dd2-4f52-a405-18a8acc32f2a",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca1bc771-b659-4e76-8a30-3e1849c5d484",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(r\"C:\\Users\\Arman\\Downloads\\dataset\\logit classification.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cb965ea-41d5-4672-8c0e-76163b04fa37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User ID</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Purchased</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15624510</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>19000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15810944</td>\n",
       "      <td>Male</td>\n",
       "      <td>35</td>\n",
       "      <td>20000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15668575</td>\n",
       "      <td>Female</td>\n",
       "      <td>26</td>\n",
       "      <td>43000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15603246</td>\n",
       "      <td>Female</td>\n",
       "      <td>27</td>\n",
       "      <td>57000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15804002</td>\n",
       "      <td>Male</td>\n",
       "      <td>19</td>\n",
       "      <td>76000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>15691863</td>\n",
       "      <td>Female</td>\n",
       "      <td>46</td>\n",
       "      <td>41000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>15706071</td>\n",
       "      <td>Male</td>\n",
       "      <td>51</td>\n",
       "      <td>23000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>15654296</td>\n",
       "      <td>Female</td>\n",
       "      <td>50</td>\n",
       "      <td>20000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>15755018</td>\n",
       "      <td>Male</td>\n",
       "      <td>36</td>\n",
       "      <td>33000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>15594041</td>\n",
       "      <td>Female</td>\n",
       "      <td>49</td>\n",
       "      <td>36000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      User ID  Gender  Age  EstimatedSalary  Purchased\n",
       "0    15624510    Male   19            19000          0\n",
       "1    15810944    Male   35            20000          0\n",
       "2    15668575  Female   26            43000          0\n",
       "3    15603246  Female   27            57000          0\n",
       "4    15804002    Male   19            76000          0\n",
       "..        ...     ...  ...              ...        ...\n",
       "395  15691863  Female   46            41000          1\n",
       "396  15706071    Male   51            23000          1\n",
       "397  15654296  Female   50            20000          1\n",
       "398  15755018    Male   36            33000          0\n",
       "399  15594041  Female   49            36000          1\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bb7b15-d2a2-4fc2-bc37-c35c631c3ea3",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "## Split into features (X) and target (y)\n",
    "- X: Features (Age, EstimatedSalary)\n",
    "- y: Target (Purchased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ea6ba8-2884-46e3-9920-18637d8ce186",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[[\"Age\", \"EstimatedSalary\"]].values\n",
    "y = dataset[\"Purchased\"].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766fa71d-c18e-498b-a686-b57113d7f722",
   "metadata": {},
   "source": [
    "## Splitting the dataset into the Training set and Test set¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1223ec7c-7426-4842-abab-0bfce5a16869",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a706057-2c19-4e41-83dc-c4e822f89ea2",
   "metadata": {},
   "source": [
    "## Feature Scaling\n",
    "## Apply StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1eb5cba-4052-488d-98ac-3dd8bc386667",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler() \n",
    "X_train_sc = sc.fit_transform(X_train)\n",
    "X_test_sc = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3cab1f-14c4-4e47-b3d2-28f1b572173b",
   "metadata": {},
   "source": [
    "## Training and Evaluating Decision Tree Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1e2afd-df88-4ec4-a442-9d64a5276b06",
   "metadata": {},
   "source": [
    "## With max_depth (3) Without Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0167f76-6bed-4807-9594-b4da74e188ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (max_depth=3) without Scaling\n",
      "Accuracy: 0.94\n",
      "Confusion Matrix:\n",
      " [[64  4]\n",
      " [ 2 30]]\n"
     ]
    }
   ],
   "source": [
    "classifier1 = DecisionTreeClassifier(max_depth=3)\n",
    "classifier1.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = classifier1.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree (max_depth=3) without Scaling\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred1))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5a7fe2-84ca-4b71-8c47-412d30523f06",
   "metadata": {},
   "source": [
    "## With max_depth (10) Without Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "654b279f-ca85-4559-8a9c-04abb3492050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (max_depth=10) without Scaling\n",
      "Accuracy: 0.92\n",
      "Confusion Matrix:\n",
      " [[64  4]\n",
      " [ 4 28]]\n"
     ]
    }
   ],
   "source": [
    "classifier2 = DecisionTreeClassifier(max_depth=10)\n",
    "classifier2.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = classifier2.predict(X_test)\n",
    "\n",
    "print(\"Decision Tree (max_depth=10) without Scaling\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred2))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ef0e47-39ac-4df1-b688-677769cefded",
   "metadata": {},
   "source": [
    "## With max_depth (3) With Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cab35af5-3c06-4d4f-b1c8-ed388cb11612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (max_depth=3) with Scaling\n",
      "Accuracy: 0.94\n",
      "Confusion Matrix:\n",
      " [[64  4]\n",
      " [ 2 30]]\n"
     ]
    }
   ],
   "source": [
    "classifier3 = DecisionTreeClassifier(max_depth=3)\n",
    "classifier3.fit(X_train_sc, y_train)\n",
    "\n",
    "y_pred3 = classifier3.predict(X_test_sc)\n",
    "\n",
    "print(\"Decision Tree (max_depth=3) with Scaling\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred3))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ede37b-ad63-422a-8f59-5dd4e7da51f0",
   "metadata": {},
   "source": [
    "## With max_depth (10) With Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2eeabce0-582c-4964-934d-cd9ce3d76796",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree (max_depth=10) with Scaling\n",
      "Accuracy: 0.93\n",
      "Confusion Matrix:\n",
      " [[64  4]\n",
      " [ 3 29]]\n"
     ]
    }
   ],
   "source": [
    "classifier4 = DecisionTreeClassifier(max_depth=10)\n",
    "classifier4.fit(X_train_sc, y_train)\n",
    "\n",
    "y_pred4 = classifier4.predict(X_test_sc)\n",
    "\n",
    "print(\"Decision Tree (max_depth=10) with Scaling\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred4))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6943973e-7580-4fdd-9fb0-a512dd54f131",
   "metadata": {},
   "source": [
    "---\n",
    "## Interpretation\n",
    "\n",
    "From the results, we observe that **Decision Tree Classifier does not require feature scaling**.  \n",
    "- Accuracy remains the same with and without scaling (0.94 at depth=3, 0.93–0.92 at depth=10).  \n",
    "- This is because decision trees split data based on feature thresholds (e.g., Age > 30), and these thresholds are **not affected by scaling**.  \n",
    "\n",
    "When comparing different tree depths:  \n",
    "- **Max Depth = 3:** Achieved the best performance (Accuracy = 0.94) with balanced confusion matrix results.  \n",
    "- **Max Depth = 10:** Accuracy slightly decreased (0.92–0.93), showing that very deep trees may **overfit** the training data and generalize worse.  \n",
    "\n",
    "**Conclusion:** Decision Trees are scale-invariant, and controlling the tree depth is more important than scaling for good performance.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2bbafa-dcca-4153-ac33-53e356bfe167",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, I studied the **Decision Tree Classifier**, beginning with the theoretical foundation of entropy, information gain, Gini impurity, and pruning. I then implemented and evaluated decision trees on the dataset under different settings:  \n",
    "- With and without scaling  \n",
    "- With maximum depths of 3 and 10  \n",
    "\n",
    "The results showed that **feature scaling does not affect decision tree performance**, as accuracy remained the same in both scaled and unscaled data. However, tree depth had a direct impact on performance. A shallow tree (max_depth=3) provided slightly better generalization, while a deeper tree (max_depth=10) led to a minor drop in test accuracy due to overfitting.\n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- **Decision Trees do not require feature scaling**, since splits are based on raw threshold values of features, not distances.  \n",
    "- **Entropy and Information Gain** (or Gini) are used to decide the best splits in building the tree.  \n",
    "- **Tree depth matters**:  \n",
    "  - Shallow trees (e.g., depth=3) generalize better.  \n",
    "  - Deeper trees (e.g., depth=10) may overfit and reduce test accuracy.  \n",
    "- **Pruning parameters** like `max_depth`, `min_samples_split`, and `min_samples_leaf` are essential to prevent overfitting.  \n",
    "- Decision Trees are **easy to interpret and visualize**, making them highly useful for understanding model decisions.  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
