{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3206d18-5d04-4e90-94f5-5973ba92c13f",
   "metadata": {},
   "source": [
    "# Day 65 – Introduction to spaCy Library\n",
    "\n",
    "In this session, I learned about the **spaCy library**, one of the most powerful tools for **Natural Language Processing (NLP)** in Python.\n",
    "I explored how spaCy processes text using its **efficient NLP pipeline**, covering key concepts like **tokenization**, **part-of-speech tagging**, **named entity recognition (NER)**, and **dependency parsing**.\n",
    "\n",
    "The goal of this session was to understand **how spaCy simplifies complex text processing tasks** and prepares data for **real-world NLP applications**.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Introduction to spaCy\n",
    "\n",
    "**spaCy** is one of the most powerful and efficient **Natural Language Processing (NLP)** libraries in Python, designed specifically for **industrial and real-world applications**.\n",
    "It provides fast, accurate, and easy-to-use tools for text processing and linguistic analysis.\n",
    "\n",
    "Unlike older libraries such as **NLTK**, spaCy focuses on **production-ready performance**, offering pre-trained pipelines for multiple languages.\n",
    "\n",
    "---\n",
    "\n",
    "## Why spaCy?\n",
    "\n",
    "spaCy is preferred by developers and data scientists because of its:\n",
    "\n",
    "* **Speed:** Written in Cython (a blend of Python and C), making it extremely fast.\n",
    "* **Ease of Use:** Clean, consistent API for working with NLP pipelines.\n",
    "* **Pre-trained Models:** Ready-to-use models for tokenization, POS tagging, NER, and dependency parsing.\n",
    "* **Production Focus:** Designed for building real-world applications like chatbots, extractors, and sentiment systems.\n",
    "* **Integration:** Works smoothly with libraries like **scikit-learn**, **Hugging Face**, and **PyTorch**.\n",
    "\n",
    "---\n",
    "\n",
    "## spaCy NLP Pipeline\n",
    "\n",
    "When you pass text into a spaCy model, it goes through several processing stages known as the **NLP pipeline**:\n",
    "\n",
    "1. **Tokenization** → Splitting text into meaningful units (tokens).\n",
    "2. **Part-of-Speech (POS) Tagging** → Identifying the grammatical role of each token (noun, verb, adjective, etc.).\n",
    "3. **Dependency Parsing** → Understanding how words are related grammatically within a sentence.\n",
    "4. **Named Entity Recognition (NER)** → Detecting named entities like people, places, organizations, and dates.\n",
    "5. **Lemmatization** → Converting words to their base or dictionary form.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "> “Apple is looking at buying U.K. startup for $1 billion.”\n",
    "\n",
    "* **Entities:** Apple (ORG), U.K. (GPE), $1 billion (MONEY)\n",
    "\n",
    "---\n",
    "\n",
    "## Key Components in spaCy\n",
    "\n",
    "| Component   | Description                             | Example Output                   |\n",
    "| ----------- | --------------------------------------- | -------------------------------- |\n",
    "| **Doc**     | Processed text returned by the pipeline | Entire sentence/document         |\n",
    "| **Token**   | Individual word, punctuation, or symbol | “Apple”, “is”, “looking”         |\n",
    "| **Span**    | Slice of a Doc (subset of tokens)       | “Apple is looking”               |\n",
    "| **Vocab**   | Stores word information and vectors     | Contains lexical attributes      |\n",
    "| **Matcher** | Rule-based pattern matching engine      | Detects patterns like “New York” |\n",
    "\n",
    "---\n",
    "\n",
    "## Common spaCy Functionalities\n",
    "\n",
    "* **Tokenization:** Breaks sentences into words.\n",
    "* **Stopword Removal:** Filters out common uninformative words (e.g., “the”, “is”).\n",
    "* **Lemmatization:** Converts words to their base form (“running” → “run”).\n",
    "* **POS Tagging:** Identifies grammatical category (noun, verb, etc.).\n",
    "* **Named Entity Recognition (NER):** Detects entities like names, locations, money, etc.\n",
    "* **Dependency Parsing:** Understands how words relate syntactically.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparison: spaCy vs NLTK\n",
    "\n",
    "| Feature               | **spaCy**                      | **NLTK**             |\n",
    "| --------------------- | ------------------------------ | -------------------- |\n",
    "| Purpose               | Industrial / Production        | Research / Teaching  |\n",
    "| Speed                 | Very fast (Cython-based)       | Slower (pure Python) |\n",
    "| Ease of Use           | Clean API                      | More complex setup   |\n",
    "| Deep Learning Support | Yes (via `spacy-transformers`) | Limited              |\n",
    "| Pre-trained Models    | Built-in                       | Mostly user-trained  |\n",
    "| Focus                 | Modern, real-world NLP         | Academic learning    |\n",
    "\n",
    "---\n",
    "\n",
    "## Applications of spaCy\n",
    "\n",
    "* Information extraction from documents\n",
    "* Chatbots and conversational AI\n",
    "* Sentiment analysis\n",
    "* Named Entity Recognition (NER) systems\n",
    "* Resume and document parsing\n",
    "* Text summarization and classification\n",
    "* Legal, medical, and financial text analysis\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a284bd09-f6b3-4acc-a2a7-0f5fd6287793",
   "metadata": {},
   "source": [
    "## Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d3190df-c4d4-4dd4-b395-f6aa5fb10151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCy \n",
    "#!pip install spacy\n",
    "\n",
    "# Download the English language model (small)\n",
    "#!python -m spacy download en_core_web_sm\n",
    "\n",
    "# For larger models:\n",
    "# en_core_web_md  (medium)\n",
    "# en_core_web_lg  (large)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e28ffc9-2c37-4fa8-89e6-05a02bb92be8",
   "metadata": {},
   "source": [
    "## Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ea07d44-28f3-4094-ad38-301606ed8fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Elon Musk founded SpaceX in 2002 and acquired Twitter in 2022."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "#Load the english language model\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "#Example text\n",
    "text = \"Elon Musk founded SpaceX in 2002 and acquired Twitter in 2022.\"\n",
    "\n",
    "#Process the text\n",
    "doc = nlp(text)\n",
    "\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f78a0bc-000b-45ae-a9a9-43d1f60502d6",
   "metadata": {},
   "source": [
    "## NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc38c0c2-0708-4d6b-ae80-b08e7776d21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named Entities, Phrases and Concepts:\n",
      "Elon Musk       PERSON              0          9\n",
      "2002            DATE               28         32\n",
      "Twitter         PERSON             46         53\n",
      "2022            DATE               57         61\n"
     ]
    }
   ],
   "source": [
    "# print named entities found in the text\n",
    "print(\"Named Entities, Phrases and Concepts:\")\n",
    "for ent in doc.ents:\n",
    "    print(f\"{ent.text:15} {ent.label_:10} {ent.start_char:10} {ent.end_char:10}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc27b35-78c7-48ed-beaa-cc7f022e1165",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8fbc61d0-1ef7-4024-86a2-45d339cf6f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon\n",
      "Musk\n",
      "founded\n",
      "SpaceX\n",
      "in\n",
      "2002\n",
      "and\n",
      "acquired\n",
      "Twitter\n",
      "in\n",
      "2022\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aaf2c6-d57c-4459-ba5d-f7b926b80d65",
   "metadata": {},
   "source": [
    "## Part of Speech (POS) Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd0f4c98-7c85-4acb-8790-5a3f819eae8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon : PROPN\n",
      "Musk : PROPN\n",
      "founded : VERB\n",
      "SpaceX : PROPN\n",
      "in : ADP\n",
      "2002 : NUM\n",
      "and : CCONJ\n",
      "acquired : VERB\n",
      "Twitter : PROPN\n",
      "in : ADP\n",
      "2022 : NUM\n",
      ". : PUNCT\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70851970-47d4-4770-9fc7-889fe6fece80",
   "metadata": {},
   "source": [
    "## Lemmatization & Dependency Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648dcd84-d872-41ac-9c1a-ec9c76b14f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon : PROPN --> Elon | Dependency: compound\n",
      "Musk : PROPN --> Musk | Dependency: nsubj\n",
      "founded : VERB --> found | Dependency: ROOT\n",
      "SpaceX : PROPN --> SpaceX | Dependency: dobj\n",
      "in : ADP --> in | Dependency: prep\n",
      "2002 : NUM --> 2002 | Dependency: pobj\n",
      "and : CCONJ --> and | Dependency: cc\n",
      "acquired : VERB --> acquire | Dependency: conj\n",
      "Twitter : PROPN --> Twitter | Dependency: dobj\n",
      "in : ADP --> in | Dependency: prep\n",
      "2022 : NUM --> 2022 | Dependency: pobj\n",
      ". : PUNCT --> . | Dependency: punct\n"
     ]
    }
   ],
   "source": [
    "# Tokens with POS, Lemma (base form), and Dependency relation\n",
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_, \"-->\", token.lemma_, \"| Dependency:\", token.dep_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d746c2-420c-4d84-bef3-a78c6781f131",
   "metadata": {},
   "source": [
    "## All in one: POS, Lemma, Dependency, Shape, Alphabet check, Stop word check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "139afb4f-5e20-468f-9ae3-20fd20ddfd32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elon : PROPN --> Elon | Dep: compound | Shape: Xxxx | Alpha: True | Stopword: False\n",
      "Musk : PROPN --> Musk | Dep: nsubj | Shape: Xxxx | Alpha: True | Stopword: False\n",
      "founded : VERB --> found | Dep: ROOT | Shape: xxxx | Alpha: True | Stopword: False\n",
      "SpaceX : PROPN --> SpaceX | Dep: dobj | Shape: XxxxxX | Alpha: True | Stopword: False\n",
      "in : ADP --> in | Dep: prep | Shape: xx | Alpha: True | Stopword: True\n",
      "2002 : NUM --> 2002 | Dep: pobj | Shape: dddd | Alpha: False | Stopword: False\n",
      "and : CCONJ --> and | Dep: cc | Shape: xxx | Alpha: True | Stopword: True\n",
      "acquired : VERB --> acquire | Dep: conj | Shape: xxxx | Alpha: True | Stopword: False\n",
      "Twitter : PROPN --> Twitter | Dep: dobj | Shape: Xxxxx | Alpha: True | Stopword: False\n",
      "in : ADP --> in | Dep: prep | Shape: xx | Alpha: True | Stopword: True\n",
      "2022 : NUM --> 2022 | Dep: pobj | Shape: dddd | Alpha: False | Stopword: False\n",
      ". : PUNCT --> . | Dep: punct | Shape: . | Alpha: False | Stopword: False\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, \":\", token.pos_, \"-->\", token.lemma_, \"|\", \n",
    "          \"Dep:\", token.dep_, \"| Shape:\", token.shape_, \n",
    "          \"| Alpha:\", token.is_alpha, \"| Stopword:\", token.is_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c539a658-242d-4a11-9231-167690d7b77a",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "In this session, I explored the **spaCy library**, a fast and modern NLP framework used for **linguistic analysis and text processing**.\n",
    "I learned how to load language models, process text, and perform tasks such as **tokenization**, **part-of-speech tagging**, **named entity recognition**, and **dependency parsing**.\n",
    "\n",
    "spaCy is a powerful, modern NLP toolkit designed not just for experimentation — but for building **real-world, scalable NLP applications**.\n",
    "\n",
    "spaCy’s simple and efficient API made it easy to analyze text while maintaining high speed and accuracy.\n",
    "\n",
    "\n",
    "## Key Learning\n",
    "\n",
    "* Understood what **spaCy** is and why it’s used in modern NLP workflows.\n",
    "* Learned about spaCy’s **NLP pipeline** and how it processes text step-by-step.\n",
    "* Practiced key operations like **tokenization**, **POS tagging**, **lemmatization**, and **NER**.\n",
    "* Explored how **dependency parsing** helps understand relationships between words.\n",
    "* Compared **spaCy vs NLTK**, and saw why spaCy is preferred for production use.\n",
    "* Understood that spaCy is an **industrial-grade NLP toolkit** built for real-world AI applications.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
